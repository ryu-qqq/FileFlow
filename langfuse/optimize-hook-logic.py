#!/usr/bin/env python3
"""
Hook ë¡œì§ ìë™ ìµœì í™” ìŠ¤í¬ë¦½íŠ¸

ëª©ì : Hook ì‹¤í–‰ ë¡œê·¸ë¥¼ ë¶„ì„í•˜ì—¬ í‚¤ì›Œë“œ ë§¤í•‘ ë° Layer ê°ì§€ ë¡œì§ ê°œì„ 

ì£¼ìš” ê¸°ëŠ¥:
1. í‚¤ì›Œë“œ ê°ì§€ ì •í™•ë„ ë¶„ì„
2. Layer ë§¤í•‘ íš¨ê³¼ ì¸¡ì •
3. False positive/negative ì¼€ì´ìŠ¤ ì‹ë³„
4. ê°œì„ ëœ í‚¤ì›Œë“œ ë§¤í•‘ ì œì•ˆ
5. user-prompt-submit.sh ì—…ë°ì´íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±

ì‚¬ìš©ë²•:
    # ê¸°ë³¸ ë¶„ì„ (ìµœê·¼ 100ê°œ ì„¸ì…˜)
    python3 scripts/langfuse/optimize-hook-logic.py

    # ìƒì„¸ ë¶„ì„ (ëª¨ë“  ì„¸ì…˜)
    python3 scripts/langfuse/optimize-hook-logic.py --all --verbose

    # ê°œì„  ì œì•ˆ íŒŒì¼ ìƒì„±
    python3 scripts/langfuse/optimize-hook-logic.py --output optimization-report.md
"""

import os
import sys
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from collections import Counter, defaultdict

# í”„ë¡œì íŠ¸ ë£¨íŠ¸
PROJECT_ROOT = Path(__file__).parent.parent.parent
HOOK_LOG_FILE = PROJECT_ROOT / ".claude" / "hooks" / "logs" / "hook-execution.jsonl"


class HookLogicOptimizer:
    """Hook ë¡œì§ ìµœì í™” ë¶„ì„ê¸°"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.sessions = {}
        self.keyword_stats = defaultdict(lambda: {"detected": 0, "injected": 0, "violations": 0})
        self.layer_stats = defaultdict(lambda: {"detected": 0, "injected": 0, "violations": 0})
        print("âœ… Hook ë¡œì§ ìµœì í™” ë¶„ì„ê¸° ì´ˆê¸°í™”")

    def load_hook_logs(self, max_sessions: Optional[int] = None) -> Dict:
        """
        Hook ë¡œê·¸ ë¡œë“œ

        Args:
            max_sessions: ìµœëŒ€ ì„¸ì…˜ ìˆ˜ (Noneì´ë©´ ì „ì²´)

        Returns:
            ì„¸ì…˜ë³„ ì´ë²¤íŠ¸ ë”•ì…”ë„ˆë¦¬
        """
        if not HOOK_LOG_FILE.exists():
            print(f"âš ï¸ Hook ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {HOOK_LOG_FILE}")
            return {}

        sessions = defaultdict(list)
        session_count = 0

        with open(HOOK_LOG_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    event = json.loads(line.strip())
                    sid = event.get('session_id', 'unknown')

                    # ìƒˆ ì„¸ì…˜ ê°ì§€
                    if sid not in sessions:
                        session_count += 1
                        if max_sessions and session_count > max_sessions:
                            break

                    sessions[sid].append(event)

                except json.JSONDecodeError:
                    continue

        print(f"ğŸ“Š {len(sessions)}ê°œ ì„¸ì…˜ ë¡œë“œ ì™„ë£Œ (ì´ {sum(len(events) for events in sessions.values())}ê°œ ì´ë²¤íŠ¸)")
        return sessions

    def analyze_keyword_accuracy(self, sessions: Dict) -> Dict:
        """
        í‚¤ì›Œë“œ ê°ì§€ ì •í™•ë„ ë¶„ì„

        Args:
            sessions: ì„¸ì…˜ë³„ ì´ë²¤íŠ¸

        Returns:
            í‚¤ì›Œë“œ í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        print("ğŸ” í‚¤ì›Œë“œ ê°ì§€ ì •í™•ë„ ë¶„ì„ ì¤‘...")

        keyword_usage = Counter()
        keyword_to_layer = defaultdict(Counter)
        false_positives = []
        false_negatives = []

        for sid, events in sessions.items():
            # ì´ë²¤íŠ¸ íƒ€ì…ë³„ ë¶„ë¥˜
            event_by_type = defaultdict(list)
            for e in events:
                event_type = e.get('event', 'unknown')
                event_by_type[event_type].append(e)

            # í‚¤ì›Œë“œ ë¶„ì„
            keyword_analysis = event_by_type.get('keyword_analysis', [{}])[0]
            detected_keywords = keyword_analysis.get('detected_keywords', [])
            detected_layers = keyword_analysis.get('detected_layers', [])
            context_score = keyword_analysis.get('context_score', 0)

            # Cache ì£¼ì…
            cache_injections = event_by_type.get('cache_injection', [])

            # ê²€ì¦ ê²°ê³¼
            validation_results = event_by_type.get('validation_result', [])
            violations = sum(len(vr.get('violations', [])) for vr in validation_results)

            # í‚¤ì›Œë“œ ì‚¬ìš© ì¶”ì 
            for keyword in detected_keywords:
                keyword_usage[keyword] += 1

                # í‚¤ì›Œë“œ â†’ Layer ë§¤í•‘ ì¶”ì 
                for layer in detected_layers:
                    keyword_to_layer[keyword][layer] += 1

            # False Positive: í‚¤ì›Œë“œ ê°ì§€í–ˆì§€ë§Œ Cache ì£¼ì… ì‹¤íŒ¨
            if detected_keywords and not cache_injections:
                false_positives.append({
                    "session_id": sid,
                    "keywords": detected_keywords,
                    "context_score": context_score
                })

            # False Negative: Cache ì£¼ì… ì—†ì—ˆì§€ë§Œ ìœ„ë°˜ ë°œìƒ (ê°ì§€ ì‹¤íŒ¨)
            if not cache_injections and violations > 0:
                false_negatives.append({
                    "session_id": sid,
                    "violations": violations
                })

        # í†µê³„ ê³„ì‚°
        total_sessions = len(sessions)
        total_detections = sum(keyword_usage.values())
        avg_keywords_per_session = total_detections / total_sessions if total_sessions > 0 else 0

        analysis = {
            "total_sessions": total_sessions,
            "total_keyword_detections": total_detections,
            "avg_keywords_per_session": avg_keywords_per_session,
            "keyword_frequency": dict(keyword_usage.most_common(20)),
            "keyword_to_layer_mapping": {k: dict(v) for k, v in keyword_to_layer.items()},
            "false_positives": len(false_positives),
            "false_negatives": len(false_negatives),
            "false_positive_rate": len(false_positives) / total_sessions * 100 if total_sessions > 0 else 0,
            "false_negative_rate": len(false_negatives) / total_sessions * 100 if total_sessions > 0 else 0,
            "false_positive_examples": false_positives[:5],
            "false_negative_examples": false_negatives[:5]
        }

        return analysis

    def analyze_layer_detection(self, sessions: Dict) -> Dict:
        """
        Layer ê°ì§€ íš¨ê³¼ ë¶„ì„

        Args:
            sessions: ì„¸ì…˜ë³„ ì´ë²¤íŠ¸

        Returns:
            Layer í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        print("ğŸ¯ Layer ê°ì§€ íš¨ê³¼ ë¶„ì„ ì¤‘...")

        layer_detections = Counter()
        layer_violations = Counter()
        layer_injection_success = Counter()
        layer_injection_attempts = Counter()

        for sid, events in sessions.items():
            event_by_type = defaultdict(list)
            for e in events:
                event_type = e.get('event', 'unknown')
                event_by_type[event_type].append(e)

            # Layer ê°ì§€
            keyword_analysis = event_by_type.get('keyword_analysis', [{}])[0]
            detected_layers = keyword_analysis.get('detected_layers', [])

            # Cache ì£¼ì…
            cache_injections = event_by_type.get('cache_injection', [])

            # ê²€ì¦ ê²°ê³¼
            validation_results = event_by_type.get('validation_result', [])

            # Layerë³„ í†µê³„
            for layer in detected_layers:
                layer_detections[layer] += 1

                # ì£¼ì… ì‹œë„
                layer_injection_attempts[layer] += 1

                # ì£¼ì… ì„±ê³µ ì—¬ë¶€
                layer_injected = any(ci.get('layer') == layer for ci in cache_injections)
                if layer_injected:
                    layer_injection_success[layer] += 1

            # Layerë³„ ìœ„ë°˜
            for vr in validation_results:
                layer = vr.get('layer', 'unknown')
                violations_count = len(vr.get('violations', []))
                if violations_count > 0:
                    layer_violations[layer] += violations_count

        # í†µê³„ ê³„ì‚°
        analysis = {
            "layer_detection_frequency": dict(layer_detections.most_common()),
            "layer_injection_success_rate": {
                layer: (layer_injection_success[layer] / layer_injection_attempts[layer] * 100)
                if layer_injection_attempts[layer] > 0 else 0
                for layer in layer_detections.keys()
            },
            "layer_violation_frequency": dict(layer_violations.most_common()),
            "layer_effectiveness_score": {
                layer: (
                    100 - (layer_violations[layer] / layer_detections[layer] * 100)
                    if layer_detections[layer] > 0 else 0
                )
                for layer in layer_detections.keys()
            }
        }

        return analysis

    def generate_keyword_suggestions(self, keyword_analysis: Dict, layer_analysis: Dict) -> Dict:
        """
        ê°œì„ ëœ í‚¤ì›Œë“œ ë§¤í•‘ ì œì•ˆ

        Args:
            keyword_analysis: í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼
            layer_analysis: Layer ë¶„ì„ ê²°ê³¼

        Returns:
            í‚¤ì›Œë“œ ê°œì„  ì œì•ˆ ë”•ì…”ë„ˆë¦¬
        """
        print("ğŸ’¡ í‚¤ì›Œë“œ ë§¤í•‘ ê°œì„  ì œì•ˆ ìƒì„± ì¤‘...")

        suggestions = {
            "high_value_keywords": [],
            "low_value_keywords": [],
            "new_keyword_candidates": [],
            "layer_mapping_improvements": {}
        }

        # 1. ê³ ê°€ì¹˜ í‚¤ì›Œë“œ (ê°ì§€ìœ¨ ë†’ê³  ìœ„ë°˜ìœ¨ ë‚®ìŒ)
        keyword_freq = keyword_analysis.get('keyword_frequency', {})
        for keyword, count in keyword_freq.items():
            if count >= 5:  # ìµœì†Œ 5ë²ˆ ì´ìƒ ê°ì§€
                suggestions["high_value_keywords"].append({
                    "keyword": keyword,
                    "frequency": count,
                    "recommendation": "ìœ ì§€ ë˜ëŠ” ì ìˆ˜ ì¦ê°€"
                })

        # 2. ì €ê°€ì¹˜ í‚¤ì›Œë“œ (ê°ì§€ìœ¨ ë‚®ê±°ë‚˜ False Positive ë§ìŒ)
        false_positive_rate = keyword_analysis.get('false_positive_rate', 0)
        if false_positive_rate > 10:
            suggestions["low_value_keywords"].append({
                "pattern": "generic_keywords",
                "false_positive_rate": f"{false_positive_rate:.1f}%",
                "recommendation": "ë” êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë¡œ ëŒ€ì²´"
            })

        # 3. Layer ë§¤í•‘ ê°œì„ 
        keyword_to_layer = keyword_analysis.get('keyword_to_layer_mapping', {})
        layer_effectiveness = layer_analysis.get('layer_effectiveness_score', {})

        for keyword, layers in keyword_to_layer.items():
            most_common_layer = max(layers, key=layers.get) if layers else None
            if most_common_layer:
                effectiveness = layer_effectiveness.get(most_common_layer, 0)

                if effectiveness < 80:
                    suggestions["layer_mapping_improvements"][keyword] = {
                        "current_layer": most_common_layer,
                        "effectiveness": f"{effectiveness:.1f}%",
                        "recommendation": "Layer ë§¤í•‘ ì¬ê²€í†  í•„ìš”"
                    }

        # 4. ìƒˆë¡œìš´ í‚¤ì›Œë“œ í›„ë³´ (False Negative ë¶„ì„ ê¸°ë°˜)
        false_negative_rate = keyword_analysis.get('false_negative_rate', 0)
        if false_negative_rate > 5:
            suggestions["new_keyword_candidates"].append({
                "observation": f"ìœ„ë°˜ ì‚¬ë¡€ {false_negative_rate:.1f}%ì—ì„œ í‚¤ì›Œë“œ ê°ì§€ ì‹¤íŒ¨",
                "recommendation": "ìœ„ë°˜ ì„¸ì…˜ì˜ ì‚¬ìš©ì ì…ë ¥ íŒ¨í„´ ì¬ë¶„ì„ í•„ìš”"
            })

        return suggestions

    def generate_optimization_report(
        self,
        keyword_analysis: Dict,
        layer_analysis: Dict,
        suggestions: Dict,
        output_file: Optional[str] = None
    ):
        """
        ìµœì í™” ë³´ê³ ì„œ ìƒì„±

        Args:
            keyword_analysis: í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼
            layer_analysis: Layer ë¶„ì„ ê²°ê³¼
            suggestions: ê°œì„  ì œì•ˆ
            output_file: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        """
        print("ğŸ“ ìµœì í™” ë³´ê³ ì„œ ìƒì„± ì¤‘...")

        report_lines = [
            "# Hook ë¡œì§ ìë™ ìµœì í™” ë³´ê³ ì„œ",
            "",
            f"**ìƒì„± ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "---",
            "",
            "## ğŸ“Š í‚¤ì›Œë“œ ê°ì§€ ë¶„ì„",
            "",
            f"- **ì´ ì„¸ì…˜ ìˆ˜**: {keyword_analysis['total_sessions']}",
            f"- **ì´ í‚¤ì›Œë“œ ê°ì§€**: {keyword_analysis['total_keyword_detections']}íšŒ",
            f"- **í‰ê·  í‚¤ì›Œë“œ/ì„¸ì…˜**: {keyword_analysis['avg_keywords_per_session']:.1f}ê°œ",
            f"- **False Positive ë¹„ìœ¨**: {keyword_analysis['false_positive_rate']:.1f}%",
            f"- **False Negative ë¹„ìœ¨**: {keyword_analysis['false_negative_rate']:.1f}%",
            "",
            "### í‚¤ì›Œë“œ ë¹ˆë„ (ìƒìœ„ 10ê°œ)",
            ""
        ]

        # í‚¤ì›Œë“œ ë¹ˆë„
        for keyword, count in list(keyword_analysis['keyword_frequency'].items())[:10]:
            report_lines.append(f"- **{keyword}**: {count}íšŒ")

        report_lines.extend([
            "",
            "---",
            "",
            "## ğŸ¯ Layer ê°ì§€ ë¶„ì„",
            "",
        ])

        # Layer ê°ì§€ ë¹ˆë„
        layer_freq = layer_analysis.get('layer_detection_frequency', {})
        if layer_freq:
            report_lines.append("### Layer ê°ì§€ ë¹ˆë„")
            report_lines.append("")
            for layer, count in layer_freq.items():
                report_lines.append(f"- **{layer}**: {count}íšŒ")
            report_lines.append("")

        # Layer ì£¼ì… ì„±ê³µë¥ 
        injection_rate = layer_analysis.get('layer_injection_success_rate', {})
        if injection_rate:
            report_lines.append("### Layer Cache ì£¼ì… ì„±ê³µë¥ ")
            report_lines.append("")
            for layer, rate in injection_rate.items():
                report_lines.append(f"- **{layer}**: {rate:.1f}%")
            report_lines.append("")

        # Layer íš¨ê³¼ì„± ì ìˆ˜
        effectiveness = layer_analysis.get('layer_effectiveness_score', {})
        if effectiveness:
            report_lines.append("### Layer íš¨ê³¼ì„± ì ìˆ˜ (ìœ„ë°˜ ê°ì†Œìœ¨)")
            report_lines.append("")
            for layer, score in effectiveness.items():
                status = "âœ…" if score >= 80 else "âš ï¸" if score >= 60 else "âŒ"
                report_lines.append(f"- {status} **{layer}**: {score:.1f}%")
            report_lines.append("")

        report_lines.extend([
            "---",
            "",
            "## ğŸ’¡ ê°œì„  ì œì•ˆ",
            "",
        ])

        # ê³ ê°€ì¹˜ í‚¤ì›Œë“œ
        high_value = suggestions.get('high_value_keywords', [])
        if high_value:
            report_lines.append("### ğŸ¯ ê³ ê°€ì¹˜ í‚¤ì›Œë“œ (ìœ ì§€ ê¶Œì¥)")
            report_lines.append("")
            for kw in high_value[:5]:
                report_lines.append(f"- **{kw['keyword']}** ({kw['frequency']}íšŒ): {kw['recommendation']}")
            report_lines.append("")

        # ì €ê°€ì¹˜ í‚¤ì›Œë“œ
        low_value = suggestions.get('low_value_keywords', [])
        if low_value:
            report_lines.append("### âš ï¸ ì €ê°€ì¹˜ í‚¤ì›Œë“œ (ê²€í†  í•„ìš”)")
            report_lines.append("")
            for kw in low_value:
                report_lines.append(f"- **{kw['pattern']}** (FP: {kw['false_positive_rate']}): {kw['recommendation']}")
            report_lines.append("")

        # Layer ë§¤í•‘ ê°œì„ 
        mapping_improvements = suggestions.get('layer_mapping_improvements', {})
        if mapping_improvements:
            report_lines.append("### ğŸ”„ Layer ë§¤í•‘ ê°œì„ ")
            report_lines.append("")
            for keyword, info in list(mapping_improvements.items())[:5]:
                report_lines.append(
                    f"- **{keyword}** â†’ {info['current_layer']} (íš¨ê³¼ì„±: {info['effectiveness']}): "
                    f"{info['recommendation']}"
                )
            report_lines.append("")

        # ìƒˆ í‚¤ì›Œë“œ í›„ë³´
        new_keywords = suggestions.get('new_keyword_candidates', [])
        if new_keywords:
            report_lines.append("### ğŸ†• ìƒˆ í‚¤ì›Œë“œ í›„ë³´")
            report_lines.append("")
            for candidate in new_keywords:
                report_lines.append(f"- **ê´€ì°°**: {candidate['observation']}")
                report_lines.append(f"  - **ê¶Œì¥**: {candidate['recommendation']}")
            report_lines.append("")

        report_lines.extend([
            "---",
            "",
            "## ğŸš€ ë‹¤ìŒ ë‹¨ê³„",
            "",
            "1. **ê³ ê°€ì¹˜ í‚¤ì›Œë“œ ê°•í™”**: ì ìˆ˜ë¥¼ 30 â†’ 40ìœ¼ë¡œ ì¦ê°€",
            "2. **ì €ê°€ì¹˜ í‚¤ì›Œë“œ ì œê±°**: False Positive ë§ì€ í‚¤ì›Œë“œ ì‚­ì œ",
            "3. **Layer ë§¤í•‘ ì¬ê²€í† **: íš¨ê³¼ì„± 80% ë¯¸ë§Œì¸ ë§¤í•‘ ê°œì„ ",
            "4. **ìƒˆ í‚¤ì›Œë“œ ì¶”ê°€**: False Negative ì‚¬ë¡€ì—ì„œ íŒ¨í„´ ë°œê²¬",
            "5. **A/B í…ŒìŠ¤íŠ¸**: ê°œì„ ëœ Hook ë¡œì§ íš¨ê³¼ ê²€ì¦",
            "",
            "---",
            ""
        ])

        report_text = "\n".join(report_lines)

        # ì¶œë ¥
        if output_file:
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report_text)
            print(f"âœ… ë³´ê³ ì„œ ì €ì¥: {output_path}")
        else:
            print("\n" + report_text)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description="Hook ë¡œì§ ìë™ ìµœì í™”")
    parser.add_argument('--all', action='store_true', help="ëª¨ë“  ì„¸ì…˜ ë¶„ì„ (ê¸°ë³¸: ìµœê·¼ 100ê°œ)")
    parser.add_argument('--sessions', type=int, default=100, help="ë¶„ì„í•  ì„¸ì…˜ ìˆ˜")
    parser.add_argument('--verbose', action='store_true', help="ìƒì„¸ ì¶œë ¥")
    parser.add_argument('--output', type=str, help="ë³´ê³ ì„œ ì¶œë ¥ íŒŒì¼ (Markdown)")

    args = parser.parse_args()

    try:
        optimizer = HookLogicOptimizer()

        # ë¡œê·¸ ë¡œë“œ
        max_sessions = None if args.all else args.sessions
        sessions = optimizer.load_hook_logs(max_sessions=max_sessions)

        if not sessions:
            print("âš ï¸ ë¶„ì„í•  ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ë¶„ì„ ì‹¤í–‰
        keyword_analysis = optimizer.analyze_keyword_accuracy(sessions)
        layer_analysis = optimizer.analyze_layer_detection(sessions)
        suggestions = optimizer.generate_keyword_suggestions(keyword_analysis, layer_analysis)

        # ë³´ê³ ì„œ ìƒì„±
        optimizer.generate_optimization_report(
            keyword_analysis,
            layer_analysis,
            suggestions,
            output_file=args.output
        )

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
