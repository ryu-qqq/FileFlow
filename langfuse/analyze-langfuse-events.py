#!/usr/bin/env python3
"""
LangFuse Events ìë™ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

ëª©ì : LangFuseì—ì„œ Hook ì‹¤í–‰ ì´ë²¤íŠ¸ë¥¼ ì¡°íšŒí•˜ì—¬ ìœ„ë°˜ íŒ¨í„´ ë¶„ì„

ì£¼ìš” ê¸°ëŠ¥:
1. LangFuse Events ì¡°íšŒ (hook-execution-*)
2. ìœ„ë°˜ íŒ¨í„´ ë¶„ì„:
   - Layerë³„ ìœ„ë°˜ ê±´ìˆ˜
   - ê·œì¹™ë³„ ìœ„ë°˜ ë¹ˆë„
   - ì‹œê°„ëŒ€ë³„ íŠ¸ë Œë“œ
3. Serena & Skills ì‚¬ìš© íŒ¨í„´ ë¶„ì„
4. ê°œì„  ì œì•ˆ ìƒì„±

ì‚¬ìš©ë²•:
    # ìµœê·¼ 7ì¼ ë°ì´í„° ë¶„ì„
    python3 scripts/langfuse/analyze-langfuse-events.py

    # íŠ¹ì • ê¸°ê°„ ë¶„ì„
    python3 scripts/langfuse/analyze-langfuse-events.py --days 30

    # ë³´ê³ ì„œ íŒŒì¼ ìƒì„±
    python3 scripts/langfuse/analyze-langfuse-events.py --output report.md
"""

import os
import sys
import json
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from collections import defaultdict, Counter
from langfuse import Langfuse
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸
PROJECT_ROOT = Path(__file__).parent.parent.parent
load_dotenv(PROJECT_ROOT / ".env")


class LangFuseAnalyzer:
    """LangFuse Events ë¶„ì„ê¸°"""

    def __init__(self):
        """LangFuse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        public_key = os.getenv('LANGFUSE_PUBLIC_KEY')
        secret_key = os.getenv('LANGFUSE_SECRET_KEY')
        host = os.getenv('LANGFUSE_HOST', 'https://us.cloud.langfuse.com')

        if not public_key or not secret_key:
            raise ValueError("LANGFUSE_PUBLIC_KEY ë° LANGFUSE_SECRET_KEY í™˜ê²½ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤")

        self.langfuse = Langfuse(
            public_key=public_key,
            secret_key=secret_key,
            host=host
        )
        print(f"âœ… LangFuse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (Host: {host})")

    def fetch_hook_events(self, days: int = 7) -> List[Dict]:
        """
        LangFuseì—ì„œ Hook ì‹¤í–‰ ì´ë²¤íŠ¸ ì¡°íšŒ

        Args:
            days: ì¡°íšŒ ê¸°ê°„ (ì¼ìˆ˜)

        Returns:
            Hook ì´ë²¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸ“Š ìµœê·¼ {days}ì¼ê°„ì˜ Hook ì´ë²¤íŠ¸ ì¡°íšŒ ì¤‘...")

        # ì‹œì‘ ë‚ ì§œ ê³„ì‚°
        start_date = datetime.now() - timedelta(days=days)

        # LangFuse Events ì¡°íšŒ
        # Note: LangFuse SDK v2.xì—ì„œëŠ” client.get_events() ë©”ì„œë“œê°€ ì—†ì„ ìˆ˜ ìˆìŒ
        # ëŒ€ì‹  LangFuse APIë¥¼ ì§ì ‘ í˜¸ì¶œí•´ì•¼ í•  ìˆ˜ ìˆìŒ
        # ì—¬ê¸°ì„œëŠ” placeholderë¡œ ì‘ì„±

        events = []

        # TODO: LangFuse API ì§ì ‘ í˜¸ì¶œë¡œ êµ¬í˜„
        # GET https://us.cloud.langfuse.com/api/public/events
        # Filter: name startswith "hook-execution-"
        # Filter: timestamp >= start_date

        print(f"âš ï¸ ì£¼ì˜: LangFuse SDK v2.xì—ì„œëŠ” Events ì¡°íšŒ APIê°€ ì œê³µë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print(f"   ëŒ€ì‹  LangFuse Dashboardì—ì„œ ìˆ˜ë™ìœ¼ë¡œ ì¡°íšŒí•˜ê±°ë‚˜,")
        print(f"   REST APIë¥¼ ì§ì ‘ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.")

        return events

    def analyze_violations(self, events: List[Dict]) -> Dict:
        """
        ìœ„ë°˜ íŒ¨í„´ ë¶„ì„

        Args:
            events: Hook ì´ë²¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print("ğŸ” ìœ„ë°˜ íŒ¨í„´ ë¶„ì„ ì¤‘...")

        analysis = {
            "total_sessions": len(events),
            "total_violations": 0,
            "violations_by_layer": Counter(),
            "zero_tolerance_compliance_rate": 0.0,
            "cache_injection_success_rate": 0.0,
            "serena_usage_stats": {
                "total_memory_loads": 0,
                "total_tool_usage": 0,
                "avg_tokens_per_load": 0
            },
            "skills_usage_stats": {
                "total_skills_detected": 0,
                "skills_frequency": Counter()
            }
        }

        total_violations_count = 0
        sessions_with_violations = 0
        cache_successes = 0

        for event in events:
            output = event.get('output', {})
            metadata = event.get('metadata', {})

            # ìœ„ë°˜ ê±´ìˆ˜ ì§‘ê³„
            violations = output.get('total_violations', 0)
            total_violations_count += violations

            if violations > 0:
                sessions_with_violations += 1

            # Layerë³„ ìœ„ë°˜ (metadataì—ì„œ ì¶”ì¶œ)
            layers_injected = output.get('layers_injected', 0)
            if violations > 0 and layers_injected > 0:
                # ê°„ì ‘ì ìœ¼ë¡œ ì¶”ë¡ : ì–´ë–¤ layerì—ì„œ ìœ„ë°˜ì´ ë°œìƒí–ˆëŠ”ì§€
                # (ì‹¤ì œ ë°ì´í„°ì—ëŠ” layerë³„ ì„¸ë¶„í™”ê°€ í•„ìš”)
                analysis["violations_by_layer"]["unknown"] += violations

            # Cache ì£¼ì… ì„±ê³µë¥ 
            if output.get('cache_injection_success', False):
                cache_successes += 1

            # Serena ì‚¬ìš© í†µê³„
            if output.get('serena_memory_loaded', False):
                analysis["serena_usage_stats"]["total_memory_loads"] += 1
                analysis["serena_usage_stats"]["total_tool_usage"] += output.get('serena_tool_usage_count', 0)
                tokens = output.get('serena_total_memory_tokens', 0)
                if tokens > 0:
                    analysis["serena_usage_stats"]["avg_tokens_per_load"] += tokens

            # Skills ì‚¬ìš© í†µê³„
            skills_count = output.get('skills_detected_count', 0)
            if skills_count > 0:
                analysis["skills_usage_stats"]["total_skills_detected"] += skills_count
                skills_used = output.get('skills_used', [])
                for skill in skills_used:
                    analysis["skills_usage_stats"]["skills_frequency"][skill] += 1

        # í‰ê·  ê³„ì‚°
        if len(events) > 0:
            analysis["total_violations"] = total_violations_count
            analysis["zero_tolerance_compliance_rate"] = (
                (len(events) - sessions_with_violations) / len(events) * 100
            )
            analysis["cache_injection_success_rate"] = (
                cache_successes / len(events) * 100
            )

            if analysis["serena_usage_stats"]["total_memory_loads"] > 0:
                analysis["serena_usage_stats"]["avg_tokens_per_load"] = (
                    analysis["serena_usage_stats"]["avg_tokens_per_load"] /
                    analysis["serena_usage_stats"]["total_memory_loads"]
                )

        return analysis

    def generate_report(self, analysis: Dict, output_file: Optional[str] = None):
        """
        ë¶„ì„ ë³´ê³ ì„œ ìƒì„±

        Args:
            analysis: ë¶„ì„ ê²°ê³¼
            output_file: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ stdout)
        """
        print("ğŸ“ ë³´ê³ ì„œ ìƒì„± ì¤‘...")

        report_lines = [
            "# LangFuse Hook ì‹œìŠ¤í…œ ë¶„ì„ ë³´ê³ ì„œ",
            "",
            f"**ìƒì„± ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "---",
            "",
            "## ğŸ“Š ì „ì²´ í†µê³„",
            "",
            f"- **ì´ ì„¸ì…˜ ìˆ˜**: {analysis['total_sessions']}",
            f"- **ì´ ìœ„ë°˜ ê±´ìˆ˜**: {analysis['total_violations']}",
            f"- **Zero-Tolerance ì¤€ìˆ˜ìœ¨**: {analysis['zero_tolerance_compliance_rate']:.1f}%",
            f"- **Cache ì£¼ì… ì„±ê³µë¥ **: {analysis['cache_injection_success_rate']:.1f}%",
            "",
            "---",
            "",
            "## ğŸš¨ ìœ„ë°˜ íŒ¨í„´ ë¶„ì„",
            "",
        ]

        # Layerë³„ ìœ„ë°˜
        if analysis['violations_by_layer']:
            report_lines.append("### Layerë³„ ìœ„ë°˜ ê±´ìˆ˜")
            report_lines.append("")
            for layer, count in analysis['violations_by_layer'].most_common():
                report_lines.append(f"- **{layer}**: {count}íšŒ")
            report_lines.append("")
        else:
            report_lines.append("âœ… ìœ„ë°˜ ì‚¬í•­ ì—†ìŒ")
            report_lines.append("")

        # Serena ì‚¬ìš© í†µê³„
        report_lines.extend([
            "---",
            "",
            "## ğŸ§  Serena Memory ì‚¬ìš© í†µê³„",
            "",
            f"- **Memory ë¡œë“œ íšŸìˆ˜**: {analysis['serena_usage_stats']['total_memory_loads']}",
            f"- **Serena ë„êµ¬ ì‚¬ìš© íšŸìˆ˜**: {analysis['serena_usage_stats']['total_tool_usage']}",
            f"- **í‰ê·  í† í° ìˆ˜/ë¡œë“œ**: {analysis['serena_usage_stats']['avg_tokens_per_load']:.0f}",
            ""
        ])

        # Skills ì‚¬ìš© í†µê³„
        report_lines.extend([
            "---",
            "",
            "## ğŸ¯ Skills ì‚¬ìš© í†µê³„",
            "",
            f"- **ì´ Skills ê°ì§€ íšŸìˆ˜**: {analysis['skills_usage_stats']['total_skills_detected']}",
            ""
        ])

        if analysis['skills_usage_stats']['skills_frequency']:
            report_lines.append("### Skillë³„ ì‚¬ìš© ë¹ˆë„")
            report_lines.append("")
            for skill, count in analysis['skills_usage_stats']['skills_frequency'].most_common():
                report_lines.append(f"- **{skill}**: {count}íšŒ")
            report_lines.append("")

        # ê°œì„  ì œì•ˆ
        report_lines.extend([
            "---",
            "",
            "## ğŸ’¡ ê°œì„  ì œì•ˆ",
            "",
        ])

        # Zero-Tolerance ì¤€ìˆ˜ìœ¨ ê¸°ë°˜ ì œì•ˆ
        compliance_rate = analysis['zero_tolerance_compliance_rate']
        if compliance_rate < 80:
            report_lines.append(f"âš ï¸ **Zero-Tolerance ì¤€ìˆ˜ìœ¨ì´ {compliance_rate:.1f}%ë¡œ ë‚®ìŠµë‹ˆë‹¤.**")
            report_lines.append("   - í”„ë¡¬í”„íŠ¸ ê°•í™” í•„ìš”")
            report_lines.append("   - ìœ„ë°˜ ê·œì¹™ ì¬êµìœ¡")
            report_lines.append("")
        elif compliance_rate < 95:
            report_lines.append(f"âš ï¸ **Zero-Tolerance ì¤€ìˆ˜ìœ¨ì´ {compliance_rate:.1f}%ì…ë‹ˆë‹¤.**")
            report_lines.append("   - ì¶”ê°€ ê°œì„  ì—¬ì§€ ìˆìŒ")
            report_lines.append("")
        else:
            report_lines.append(f"âœ… **Zero-Tolerance ì¤€ìˆ˜ìœ¨ì´ {compliance_rate:.1f}%ë¡œ ìš°ìˆ˜í•©ë‹ˆë‹¤.**")
            report_lines.append("")

        # Cache ì£¼ì… ì„±ê³µë¥  ê¸°ë°˜ ì œì•ˆ
        cache_rate = analysis['cache_injection_success_rate']
        if cache_rate < 80:
            report_lines.append(f"âš ï¸ **Cache ì£¼ì… ì„±ê³µë¥ ì´ {cache_rate:.1f}%ë¡œ ë‚®ìŠµë‹ˆë‹¤.**")
            report_lines.append("   - í‚¤ì›Œë“œ ë§¤í•‘ ê²€í†  í•„ìš”")
            report_lines.append("   - Layer ê°ì§€ ë¡œì§ ê°œì„ ")
            report_lines.append("")

        report_text = "\n".join(report_lines)

        # ì¶œë ¥
        if output_file:
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report_text)
            print(f"âœ… ë³´ê³ ì„œ ì €ì¥: {output_path}")
        else:
            print("\n" + report_text)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description="LangFuse Events ìë™ ë¶„ì„")
    parser.add_argument('--days', type=int, default=7, help="ì¡°íšŒ ê¸°ê°„ (ì¼ìˆ˜)")
    parser.add_argument('--output', type=str, help="ë³´ê³ ì„œ ì¶œë ¥ íŒŒì¼ (Markdown)")

    args = parser.parse_args()

    try:
        analyzer = LangFuseAnalyzer()

        # Events ì¡°íšŒ
        events = analyzer.fetch_hook_events(days=args.days)

        if not events:
            print("âš ï¸ ì¡°íšŒëœ ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            print("   LangFuse Dashboardì—ì„œ ìˆ˜ë™ìœ¼ë¡œ í™•ì¸í•˜ì„¸ìš”:")
            print("   https://us.cloud.langfuse.com/project/claude-spring-standards")
            return

        # ë¶„ì„
        analysis = analyzer.analyze_violations(events)

        # ë³´ê³ ì„œ ìƒì„±
        analyzer.generate_report(analysis, output_file=args.output)

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
