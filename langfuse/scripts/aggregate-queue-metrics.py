#!/usr/bin/env python3
"""
Queue + Hook í†µí•© ë©”íŠ¸ë¦­ ì§‘ê³„ ë° LangFuse ì—…ë¡œë“œ

Purpose:
1. Hook ë¡œê·¸ (hook-execution.jsonl) íŒŒì‹±
2. í ë°ì´í„° (work-queue.json) íŒŒì‹±
3. ë‘ ë°ì´í„° ë³‘í•©í•˜ì—¬ ì™„ì „í•œ ê°œë°œ íš¨ìœ¨ ë¶„ì„
4. LangFuse Ingestion APIë¡œ ì—…ë¡œë“œ

Usage:
    # ë°ì´í„° ì§‘ê³„ë§Œ (ì—…ë¡œë“œ X)
    python3 aggregate-queue-metrics.py --dry-run

    # ì§‘ê³„ + LangFuse ì—…ë¡œë“œ
    python3 aggregate-queue-metrics.py

    # íŠ¹ì • ì„¸ì…˜ë§Œ ë¶„ì„
    python3 aggregate-queue-metrics.py --session-id abc123
"""

import json
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import requests
from requests.auth import HTTPBasicAuth

# ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent.parent
HOOK_LOG_FILE = PROJECT_ROOT / ".claude" / "hooks" / "logs" / "hook-execution.jsonl"
QUEUE_FILE = PROJECT_ROOT / ".claude" / "work-queue.json"
OUTPUT_FILE = PROJECT_ROOT / "langfuse" / "data" / "queue-metrics.json"


class QueueMetricsAggregator:
    """í ë©”íŠ¸ë¦­ + Hook ë¡œê·¸ í†µí•© ì§‘ê³„"""

    def __init__(self):
        self.hook_logs: List[Dict] = []
        self.queue_data: Dict = {}
        self.aggregated_data: List[Dict] = []

    def load_data(self):
        """ë°ì´í„° ë¡œë“œ"""
        # Hook ë¡œê·¸ ë¡œë“œ
        if HOOK_LOG_FILE.exists():
            self.hook_logs = []
            with open(HOOK_LOG_FILE, 'r') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        log_entry = json.loads(line)
                        self.hook_logs.append(log_entry)
                    except json.JSONDecodeError as e:
                        # íŒŒì‹± ì‹¤íŒ¨í•œ ë¼ì¸ì€ ë¬´ì‹œ
                        print(f"âš ï¸ ë¡œê·¸ íŒŒì‹± ì‹¤íŒ¨: {str(e)[:50]}...", file=sys.stderr)
                        continue
            print(f"âœ… Hook ë¡œê·¸ ë¡œë“œ: {len(self.hook_logs)}ê°œ")
        else:
            print("âš ï¸ Hook ë¡œê·¸ íŒŒì¼ ì—†ìŒ")

        # í ë°ì´í„° ë¡œë“œ
        if QUEUE_FILE.exists():
            with open(QUEUE_FILE, 'r') as f:
                self.queue_data = json.load(f)
            print(f"âœ… í ë°ì´í„° ë¡œë“œ: {len(self.queue_data.get('completed', []))}ê°œ ì™„ë£Œ ì‘ì—…")
        else:
            print("âš ï¸ í íŒŒì¼ ì—†ìŒ")

    def aggregate(self) -> List[Dict]:
        """
        Hook ë¡œê·¸ + í ë°ì´í„° ë³‘í•©

        ë³‘í•© ë¡œì§:
        1. ì„¸ì…˜ë³„ë¡œ Hook ë¡œê·¸ ê·¸ë£¹í™”
        2. ê° ì„¸ì…˜ê³¼ ë§¤ì¹­ë˜ëŠ” í ì‘ì—… ì°¾ê¸° (ì‹œê°„ ê¸°ë°˜)
        3. í†µí•© ë©”íŠ¸ë¦­ ìƒì„±
        """
        # ì„¸ì…˜ë³„ë¡œ Hook ë¡œê·¸ ê·¸ë£¹í™”
        sessions = self._group_by_session(self.hook_logs)

        # ì™„ë£Œëœ í ì‘ì—…
        completed_tasks = self.queue_data.get('completed', [])

        # ë³‘í•©
        for session_id, logs in sessions.items():
            # ì„¸ì…˜ ì‹œì‘/ì¢…ë£Œ ì‹œê°„
            start_time = logs[0]['timestamp']
            end_time = logs[-1]['timestamp']

            # ì‹œê°„ ë²”ìœ„ ë‚´ì˜ í ì‘ì—… ì°¾ê¸°
            matching_tasks = self._find_matching_tasks(
                completed_tasks, start_time, end_time
            )

            # í†µí•© ë©”íŠ¸ë¦­ ìƒì„±
            integrated_metric = self._create_integrated_metric(
                session_id, logs, matching_tasks
            )

            self.aggregated_data.append(integrated_metric)

        print(f"âœ… ì§‘ê³„ ì™„ë£Œ: {len(self.aggregated_data)}ê°œ ì„¸ì…˜")
        return self.aggregated_data

    def _group_by_session(self, logs: List[Dict]) -> Dict[str, List[Dict]]:
        """ì„¸ì…˜ IDë³„ë¡œ ë¡œê·¸ ê·¸ë£¹í™”"""
        sessions = {}
        for log in logs:
            # session_id ì¶”ì¶œ (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)
            session_id = log.get('session_id')

            # session_idê°€ ì—†ìœ¼ë©´ raw ë°ì´í„°ì—ì„œ ì¶”ì¶œ ì‹œë„
            if not session_id and 'raw' in log:
                try:
                    raw_data = json.loads(log['raw'])
                    if 'session_id' in raw_data:
                        session_id = raw_data['session_id']
                except:
                    pass

            # session_idê°€ ìˆìœ¼ë©´ ê·¸ë£¹í™”
            if session_id:
                if session_id not in sessions:
                    sessions[session_id] = []
                sessions[session_id].append(log)

        return sessions

    def _find_matching_tasks(
        self, tasks: List[Dict], start_time: str, end_time: str
    ) -> List[Dict]:
        """ì‹œê°„ ë²”ìœ„ ë‚´ì˜ í ì‘ì—… ì°¾ê¸°"""
        matching = []
        for task in tasks:
            completed_at = task.get('completed_at')
            if completed_at and start_time <= completed_at <= end_time:
                matching.append(task)
        return matching

    def _create_integrated_metric(
        self, session_id: str, logs: List[Dict], tasks: List[Dict]
    ) -> Dict:
        """í†µí•© ë©”íŠ¸ë¦­ ìƒì„±"""
        # Hook ë©”íŠ¸ë¦­ ì§‘ê³„ (event í•„ë“œ ì‚¬ìš©)
        hook_metrics = {
            'total_events': len(logs),
            'keyword_analysis_count': sum(
                1 for log in logs if log.get('event') == 'keyword_analysis'
            ),
            'cache_injection_count': sum(
                1 for log in logs if log.get('event') == 'cache_injection'
            ),
            'validation_count': sum(
                1 for log in logs if log.get('event') == 'validation_result'
            ),
            'total_rules_injected': sum(
                log.get('rules_injected', 0)
                for log in logs if log.get('event') == 'cache_injection'
            ),
            'detected_layers': list(set(
                layer
                for log in logs
                if 'detected_layers' in log
                for layer in log.get('detected_layers', [])
            ))
        }

        # í ë©”íŠ¸ë¦­ ì§‘ê³„
        queue_metrics = {
            'completed_tasks': len(tasks),
            'total_estimated_time_minutes': 0,
            'total_actual_time_minutes': 0,
            'total_code_lines': sum(task.get('code_lines', 0) for task in tasks),
            'total_files_created': sum(task.get('files_created', 0) for task in tasks),
            'total_interruptions': sum(task.get('interruptions', 0) for task in tasks),
            'average_accuracy': 0.0,
            'tasks': []
        }

        # ê° ì‘ì—…ë³„ ë©”íŠ¸ë¦­
        for task in tasks:
            estimated_minutes = self._parse_time_to_minutes(task.get('estimated_time', ''))
            actual_minutes = self._parse_time_to_minutes(task.get('actual_time', ''))

            queue_metrics['total_estimated_time_minutes'] += estimated_minutes
            queue_metrics['total_actual_time_minutes'] += actual_minutes

            queue_metrics['tasks'].append({
                'feature': task.get('feature'),
                'estimated_time': task.get('estimated_time'),
                'actual_time': task.get('actual_time'),
                'accuracy': task.get('accuracy'),
                'code_lines': task.get('code_lines', 0),
                'files_created': task.get('files_created', 0),
                'interruptions': task.get('interruptions', 0)
            })

        # í‰ê·  ì •í™•ë„ ê³„ì‚°
        accuracies = [
            float(task.get('accuracy', '0').rstrip('%'))
            for task in tasks if task.get('accuracy') and task['accuracy'] != 'N/A'
        ]
        if accuracies:
            queue_metrics['average_accuracy'] = sum(accuracies) / len(accuracies)

        # í†µí•© ë©”íŠ¸ë¦­
        return {
            'session_id': session_id,
            'timestamp': logs[0]['timestamp'],
            'session_duration': self._calculate_duration(
                logs[0]['timestamp'], logs[-1]['timestamp']
            ),
            'hook_metrics': hook_metrics,
            'queue_metrics': queue_metrics,
            'efficiency_score': self._calculate_efficiency_score(
                hook_metrics, queue_metrics
            )
        }

    def _parse_time_to_minutes(self, time_str: str) -> int:
        """ì‹œê°„ ë¬¸ìì—´ì„ ë¶„ìœ¼ë¡œ ë³€í™˜"""
        if not time_str:
            return 0

        time_str = time_str.lower().replace(" ", "")
        total_minutes = 0

        if "ì‹œê°„" in time_str:
            parts = time_str.split("ì‹œê°„")
            try:
                hours = int(parts[0])
                total_minutes += hours * 60
                if len(parts) > 1 and "ë¶„" in parts[1]:
                    mins = int(parts[1].replace("ë¶„", ""))
                    total_minutes += mins
            except ValueError:
                return 0
        elif "ë¶„" in time_str:
            try:
                mins = int(time_str.replace("ë¶„", ""))
                total_minutes = mins
            except ValueError:
                return 0

        return total_minutes

    def _calculate_duration(self, start: str, end: str) -> str:
        """ì„¸ì…˜ ì†Œìš” ì‹œê°„ ê³„ì‚°"""
        try:
            start_dt = datetime.fromisoformat(start.replace('Z', '+00:00'))
            end_dt = datetime.fromisoformat(end.replace('Z', '+00:00'))
            delta = end_dt - start_dt
            minutes = int(delta.total_seconds() / 60)

            if minutes < 60:
                return f"{minutes}ë¶„"
            else:
                hours = minutes // 60
                mins = minutes % 60
                return f"{hours}ì‹œê°„ {mins}ë¶„"
        except:
            return "N/A"

    def _calculate_efficiency_score(
        self, hook_metrics: Dict, queue_metrics: Dict
    ) -> float:
        """
        íš¨ìœ¨ì„± ì ìˆ˜ ê³„ì‚° (0-100)

        ìš”ì†Œ:
        - ê·œì¹™ ìë™ ì£¼ì… íšŸìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
        - ê²€ì¦ í†µê³¼ìœ¨
        - ì˜ˆìƒ ì‹œê°„ ì •í™•ë„
        - ì½”ë“œ ìƒì‚°ì„± (ë¼ì¸/ì‹œê°„)
        """
        score = 0.0

        # 1. ê·œì¹™ ì£¼ì… íš¨ìœ¨ (0-25ì )
        rules_per_event = (
            hook_metrics['total_rules_injected'] / hook_metrics['total_events']
            if hook_metrics['total_events'] > 0 else 0
        )
        score += min(25, rules_per_event * 2)

        # 2. ê²€ì¦ í†µê³¼ìœ¨ (0-25ì )
        validation_rate = (
            hook_metrics['validation_count'] / hook_metrics['total_events']
            if hook_metrics['total_events'] > 0 else 0
        )
        score += validation_rate * 25

        # 3. ì˜ˆìƒ ì‹œê°„ ì •í™•ë„ (0-30ì )
        score += (queue_metrics['average_accuracy'] / 100) * 30

        # 4. ì½”ë“œ ìƒì‚°ì„± (0-20ì )
        if queue_metrics['total_actual_time_minutes'] > 0:
            lines_per_hour = (
                queue_metrics['total_code_lines'] /
                (queue_metrics['total_actual_time_minutes'] / 60)
            )
            # 100 lines/hour = 20ì 
            score += min(20, (lines_per_hour / 100) * 20)

        return round(score, 1)

    def save_to_file(self):
        """ì§‘ê³„ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)

        with open(OUTPUT_FILE, 'w') as f:
            json.dump({
                'generated_at': datetime.utcnow().isoformat() + 'Z',
                'total_sessions': len(self.aggregated_data),
                'sessions': self.aggregated_data
            }, f, indent=2, ensure_ascii=False)

        print(f"âœ… ì§‘ê³„ ê²°ê³¼ ì €ì¥: {OUTPUT_FILE}")

    def upload_to_langfuse(self):
        """LangFuseë¡œ ì—…ë¡œë“œ"""
        public_key = os.getenv("LANGFUSE_PUBLIC_KEY")
        secret_key = os.getenv("LANGFUSE_SECRET_KEY")
        host = os.getenv("LANGFUSE_HOST", "https://us.cloud.langfuse.com")

        if not public_key or not secret_key:
            print("âŒ LANGFUSE_PUBLIC_KEY or LANGFUSE_SECRET_KEY not set")
            return False

        client = LangFuseUploader(public_key, secret_key, host)

        for session in self.aggregated_data:
            success = client.upload_session(session)
            if success:
                print(f"âœ… ì—…ë¡œë“œ ì„±ê³µ: {session['session_id'][:8]}...")
            else:
                print(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {session['session_id'][:8]}...")

        return True


class LangFuseUploader:
    """LangFuse Ingestion API ì—…ë¡œë”"""

    def __init__(self, public_key: str, secret_key: str, host: str):
        self.host = host.rstrip('/')
        self.session = requests.Session()
        self.session.auth = HTTPBasicAuth(public_key, secret_key)
        self.session.headers.update({'Content-Type': 'application/json'})

    def upload_session(self, session_data: Dict) -> bool:
        """ì„¸ì…˜ ë°ì´í„°ë¥¼ LangFuse Traceë¡œ ì—…ë¡œë“œ"""
        try:
            # Trace ìƒì„±
            trace_id = f"trace-{session_data['session_id'][:8]}"

            trace_batch = {
                'batch': [{
                    'id': trace_id,
                    'type': 'trace-create',
                    'timestamp': session_data['timestamp'],
                    'body': {
                        'id': trace_id,
                        'name': f"Queue Session: {session_data['queue_metrics']['completed_tasks']} tasks",
                        'timestamp': session_data['timestamp'],
                        'metadata': {
                            'session_id': session_data['session_id'],
                            'session_duration': session_data['session_duration'],
                            'efficiency_score': session_data['efficiency_score'],
                            'hook_metrics': session_data['hook_metrics'],
                            'queue_metrics': session_data['queue_metrics']
                        },
                        'tags': ['claude-code', 'queue-metrics', 'efficiency']
                    }
                }]
            }

            url = f"{self.host}/api/public/ingestion"
            response = self.session.post(url, json=trace_batch, timeout=10)
            response.raise_for_status()

            # ê° ì‘ì—…ë³„ Observation ìƒì„±
            for task in session_data['queue_metrics']['tasks']:
                self._create_task_observation(trace_id, task, session_data['timestamp'])

            return True

        except Exception as e:
            print(f"âš ï¸ LangFuse ì—…ë¡œë“œ ì‹¤íŒ¨: {e}", file=sys.stderr)
            return False

    def _create_task_observation(
        self, trace_id: str, task: Dict, base_timestamp: str
    ):
        """ì‘ì—…ë³„ Observation ìƒì„±"""
        event_id = f"{trace_id}-{task['feature']}"

        obs_batch = {
            'batch': [{
                'id': event_id,
                'type': 'event-create',
                'timestamp': base_timestamp,
                'body': {
                    'id': event_id,
                    'traceId': trace_id,
                    'name': f"Task: {task['feature']}",
                    'startTime': base_timestamp,
                    'metadata': task,
                    'level': 'DEFAULT'
                }
            }]
        }

        try:
            url = f"{self.host}/api/public/ingestion"
            response = self.session.post(url, json=obs_batch, timeout=10)
            response.raise_for_status()
        except Exception as e:
            print(f"âš ï¸ Task observation ì—…ë¡œë“œ ì‹¤íŒ¨: {e}", file=sys.stderr)


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="Queue + Hook í†µí•© ë©”íŠ¸ë¦­ ì§‘ê³„ ë° LangFuse ì—…ë¡œë“œ"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ë°ì´í„° ì§‘ê³„ë§Œ (LangFuse ì—…ë¡œë“œ X)"
    )
    parser.add_argument(
        "--session-id",
        help="íŠ¹ì • ì„¸ì…˜ë§Œ ë¶„ì„"
    )

    args = parser.parse_args()

    # ì§‘ê³„
    aggregator = QueueMetricsAggregator()
    aggregator.load_data()
    aggregator.aggregate()
    aggregator.save_to_file()

    # í†µê³„ ì¶œë ¥
    print("\nğŸ“Š ì§‘ê³„ í†µê³„:")
    print(f"  ì´ ì„¸ì…˜ ìˆ˜: {len(aggregator.aggregated_data)}ê°œ")

    total_tasks = sum(s['queue_metrics']['completed_tasks'] for s in aggregator.aggregated_data)
    total_lines = sum(s['queue_metrics']['total_code_lines'] for s in aggregator.aggregated_data)
    avg_efficiency = (
        sum(s['efficiency_score'] for s in aggregator.aggregated_data) /
        len(aggregator.aggregated_data)
        if aggregator.aggregated_data else 0
    )

    print(f"  ì™„ë£Œëœ ì‘ì—…: {total_tasks}ê°œ")
    print(f"  ìƒì„± ì½”ë“œ: {total_lines} ì¤„")
    print(f"  í‰ê·  íš¨ìœ¨ì„± ì ìˆ˜: {avg_efficiency:.1f}/100")

    # LangFuse ì—…ë¡œë“œ
    if not args.dry_run:
        print("\nğŸš€ LangFuse ì—…ë¡œë“œ ì¤‘...")
        success = aggregator.upload_to_langfuse()
        if success:
            print("âœ… ëª¨ë“  ì„¸ì…˜ ì—…ë¡œë“œ ì™„ë£Œ!")
        else:
            print("âš ï¸ ì¼ë¶€ ì„¸ì…˜ ì—…ë¡œë“œ ì‹¤íŒ¨")
    else:
        print("\nâ­ï¸  Dry-run ëª¨ë“œ: LangFuse ì—…ë¡œë“œ ìƒëµ")


if __name__ == "__main__":
    main()
