#!/usr/bin/env python3
"""
LangFuse Log Aggregator

Claude Code Hook ë¡œê·¸ ë° Pipeline ë©”íŠ¸ë¦­ì„ LangFuse Trace/Observation í˜•ì‹ìœ¼ë¡œ ë³€í™˜

Usage:
    python3 aggregate-logs.py --anonymize --output langfuse-data.json
"""

import json
import hashlib
import argparse
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional

class LangFuseAggregator:
    """Claude Code Hook ë¡œê·¸ ë° Pipeline ë©”íŠ¸ë¦­ì„ LangFuse í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""

    def __init__(self, anonymize: bool = False):
        self.anonymize = anonymize
        self.traces: Dict[str, Dict] = {}
        self.observations: List[Dict] = []
        self.session_traces: Dict[str, str] = {}  # timestamp â†’ trace_id ë§¤í•‘

    def load_claude_logs(self, log_path: str) -> None:
        """Claude Code ë¡œê·¸ ë¡œë“œ ë° ë³€í™˜"""
        if not Path(log_path).exists():
            print(f"âš ï¸  Claude logs not found: {log_path}")
            return

        with open(log_path, 'r') as f:
            for line in f:
                try:
                    event = json.loads(line.strip())
                    self._process_claude_event(event)
                except json.JSONDecodeError:
                    continue

    def load_pipeline_metrics(self, log_path: str) -> None:
        """Pipeline ë©”íŠ¸ë¦­ ë¡œë“œ ë° ë³€í™˜"""
        if not Path(log_path).exists():
            print(f"âš ï¸  Pipeline metrics not found: {log_path}")
            return

        with open(log_path, 'r') as f:
            for line in f:
                try:
                    event = json.loads(line.strip())
                    self._process_pipeline_event(event)
                except json.JSONDecodeError:
                    continue

    def _normalize_timestamp(self, timestamp: str) -> str:
        """Timestampë¥¼ LangFuse í˜¸í™˜ ISO 8601 UTC í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            # ì´ë¯¸ Zë‚˜ íƒ€ì„ì¡´ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
            if timestamp.endswith('Z') or '+' in timestamp or timestamp.count(':') > 2:
                return timestamp

            # íƒ€ì„ì¡´ ì •ë³´ ì—†ìœ¼ë©´ Z ì¶”ê°€ (UTC)
            return f"{timestamp}Z"
        except:
            # ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ (UTC)
            return datetime.utcnow().isoformat() + 'Z'

    def _process_claude_event(self, event: Dict) -> None:
        """Claude Code ì´ë²¤íŠ¸ â†’ LangFuse Observation"""
        event_type = event.get('event')
        timestamp = self._normalize_timestamp(event.get('timestamp', datetime.utcnow().isoformat()))

        if event_type == 'session_start':
            # ìƒˆ Trace ìƒì„±
            trace_id = event.get('session_id', self._generate_trace_id(event))
            self.traces[trace_id] = {
                'id': trace_id,
                'name': f"Claude Session",
                'timestamp': timestamp,
                'tags': self._extract_tags(event),
                'metadata': {
                    'project': self._anonymize_string(event.get('project', 'unknown')),
                    'user': self._anonymize_string(event.get('user', 'unknown')),
                    'tool': 'claude-code'
                }
            }
            # íƒ€ì„ìŠ¤íƒ¬í”„ â†’ trace_id ë§¤í•‘ ì €ì¥
            self.session_traces[timestamp] = trace_id

        elif event_type in ['keyword_analysis', 'cache_injection', 'validation_complete',
                            'slash_command_start', 'slash_command_complete']:
            # Observation ìƒì„±
            # NOTE: session_idëŠ” hook ë‚´ë¶€ IDë¡œ trace IDì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
            # timestampë¡œ ê°€ì¥ ê°€ê¹Œìš´ traceë¥¼ ì°¾ì•„ ì—°ê²°
            trace_id = self._find_trace_by_time(timestamp)
            if not trace_id:
                return

            self.observations.append({
                'traceId': trace_id,
                'name': self._format_event_name(event_type, event),
                'type': 'SPAN',
                'startTime': timestamp,
                'endTime': timestamp,
                'level': self._get_level(event),
                'metadata': self._extract_metadata(event),
                'tags': self._extract_tags(event)
            })

    def _process_pipeline_event(self, event: Dict) -> None:
        """Pipeline ë©”íŠ¸ë¦­ ì´ë²¤íŠ¸ â†’ LangFuse Observation"""
        timestamp = self._normalize_timestamp(event.get('timestamp', datetime.utcnow().isoformat()))
        task_name = event.get('task', 'unknown')
        status_code = event.get('status', 1)
        duration = event.get('duration', 0)

        # ê°€ì¥ ê°€ê¹Œìš´ Claude ì„¸ì…˜ ì°¾ê¸°
        trace_id = self._find_trace_by_time(timestamp)
        if not trace_id:
            # Pipeline ì „ìš© Trace ìƒì„±
            trace_id = f"pipeline-{timestamp}"
            self.traces[trace_id] = {
                'id': trace_id,
                'name': 'Pipeline Session',
                'timestamp': timestamp,
                'tags': ['pipeline'],
                'metadata': {'tool': 'pipeline'}
            }

        # ì‹œì‘ ì‹œê°„ ê³„ì‚°
        start_time = self._calculate_start_time(timestamp, duration)

        self.observations.append({
            'traceId': trace_id,
            'name': f"Pipeline: {task_name}",
            'type': 'SPAN',
            'startTime': start_time,
            'endTime': timestamp,
            'level': 'DEFAULT' if status_code == 0 else 'ERROR',
            'statusMessage': f"Exit code: {status_code}",
            'metadata': {
                'task': task_name,
                'duration_seconds': duration,
                'exit_code': status_code
            },
            'tags': ['pipeline', task_name]
        })

    def _anonymize_string(self, value: str) -> str:
        """ë¬¸ìì—´ ìµëª…í™”"""
        if not self.anonymize or not value:
            return value

        # íŒŒì¼ëª… ìµëª…í™”
        if value.endswith(('.java', '.kt', '.py')):
            return '*.java'

        # ì‚¬ìš©ìëª… ìµëª…í™” (ì´ë©”ì¼ ì œì™¸)
        if '@' not in value:
            hashed = hashlib.sha256(value.encode()).hexdigest()[:8]
            return f"user-{hashed}"

        return value

    def _extract_tags(self, event: Dict) -> List[str]:
        """ì´ë²¤íŠ¸ì—ì„œ íƒœê·¸ ì¶”ì¶œ"""
        tags = []

        if 'layer' in event:
            tags.append(event['layer'])
        if 'environment' in event:
            tags.append(event['environment'])
        if 'command' in event:
            tags.append(f"slash-command:{event['command']}")

        return tags

    def _extract_metadata(self, event: Dict) -> Dict:
        """ì´ë²¤íŠ¸ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        metadata = {}

        # ìµëª…í™”ê°€ í•„ìš”í•œ í•„ë“œ
        if 'file' in event:
            metadata['file'] = self._anonymize_string(event['file'])

        # ìˆ«ì/í†µê³„ í•„ë“œ (ìµëª…í™” ë¶ˆí•„ìš”)
        for key in ['context_score', 'rules_loaded', 'estimated_tokens',
                    'validation_time_ms', 'total_rules', 'threshold',
                    'total_rules_available']:
            if key in event:
                metadata[key] = event[key]

        # Layer ì •ë³´ (ë¶„ì„ì— ë§¤ìš° ìœ ìš©)
        if 'detected_layers' in event:
            metadata['detected_layers'] = event['detected_layers']
            metadata['layer_count'] = len(event['detected_layers'])

        if 'layer' in event:
            metadata['layer'] = event['layer']

        # í‚¤ì›Œë“œ ë¶„ì„ ì •ë³´
        if 'detected_keywords' in event:
            metadata['detected_keywords'] = event['detected_keywords']
            metadata['keyword_count'] = len(event['detected_keywords'])

        # Cache íŒŒì¼ ì •ë³´
        if 'cache_files' in event:
            metadata['cache_files_count'] = len(event['cache_files'])

        # Slash Command ì •ë³´
        if 'command' in event:
            metadata['slash_command'] = event['command']

        # íš¨ìœ¨ì„± ë©”íŠ¸ë¦­ ê³„ì‚°
        if 'rules_loaded' in event and 'total_rules_available' in event:
            total = event['total_rules_available']
            loaded = event['rules_loaded']
            if total > 0:
                metadata['rules_efficiency'] = round((loaded / total) * 100, 2)

        return metadata

    def _get_level(self, event: Dict) -> str:
        """ì´ë²¤íŠ¸ ë ˆë²¨ ê²°ì •"""
        if event.get('status') == 'failed':
            return 'ERROR'
        elif event.get('status') == 'warning':
            return 'WARNING'
        return 'DEFAULT'

    def _format_event_name(self, event_type: str, event: Dict) -> str:
        """ì´ë²¤íŠ¸ ì´ë¦„ í¬ë§·íŒ…"""
        if event_type == 'keyword_analysis':
            return "Keyword Analysis"
        elif event_type == 'cache_injection':
            layer = event.get('layer', 'unknown')
            return f"Cache Injection: {layer}"
        elif event_type == 'validation_complete':
            return "Code Validation"
        elif event_type == 'slash_command_start':
            command = event.get('command', 'unknown')
            return f"/{command}"
        elif event_type == 'slash_command_complete':
            command = event.get('command', 'unknown')
            return f"/{command} (completed)"
        return event_type

    def _find_trace_by_time(self, timestamp: str) -> Optional[str]:
        """íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ê°€ì¥ ê°€ê¹Œìš´ Trace ì°¾ê¸°"""
        if not self.session_traces:
            return None

        # ê°„ë‹¨í•œ êµ¬í˜„: ê°€ì¥ ìµœê·¼ trace ë°˜í™˜
        return list(self.traces.keys())[-1] if self.traces else None

    def _calculate_start_time(self, end_time: str, duration: float) -> str:
        """ì‹œì‘ ì‹œê°„ ê³„ì‚°"""
        try:
            end = datetime.fromisoformat(end_time.replace('Z', '+00:00'))
            start = end - timedelta(seconds=duration)
            return start.isoformat()
        except:
            return end_time

    def _generate_trace_id(self, event: Dict) -> str:
        """Trace ID ìƒì„±"""
        timestamp = event.get('timestamp', datetime.utcnow().isoformat())
        return f"session-{timestamp}"

    def export_to_langfuse(self) -> Dict:
        """LangFuse API í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        # Traceë³„ í†µê³„ ê³„ì‚°
        self._calculate_trace_statistics()

        return {
            'traces': list(self.traces.values()),
            'observations': self.observations
        }

    def _calculate_trace_statistics(self):
        """ê° Traceì˜ Observations í†µê³„ ê³„ì‚°"""
        for trace_id, trace in self.traces.items():
            # ì´ traceì— ì†í•œ observations ì°¾ê¸°
            trace_obs = [obs for obs in self.observations if obs['traceId'] == trace_id]

            if not trace_obs:
                continue

            # í†µê³„ ê³„ì‚°
            stats = {
                'total_observations': len(trace_obs),
                'total_tokens': 0,
                'total_rules_loaded': 0,
                'layers_used': set(),
                'keywords_detected': set(),
                'avg_context_score': 0
            }

            context_scores = []

            for obs in trace_obs:
                metadata = obs.get('metadata', {})

                # í† í° í•©ì‚°
                if 'estimated_tokens' in metadata:
                    stats['total_tokens'] += metadata['estimated_tokens']

                # ê·œì¹™ í•©ì‚°
                if 'rules_loaded' in metadata:
                    stats['total_rules_loaded'] += metadata['rules_loaded']

                # Layer ìˆ˜ì§‘
                if 'detected_layers' in metadata:
                    stats['layers_used'].update(metadata['detected_layers'])
                elif 'layer' in metadata:
                    stats['layers_used'].add(metadata['layer'])

                # í‚¤ì›Œë“œ ìˆ˜ì§‘
                if 'detected_keywords' in metadata:
                    stats['keywords_detected'].update(metadata['detected_keywords'])

                # Context score ìˆ˜ì§‘
                if 'context_score' in metadata:
                    context_scores.append(metadata['context_score'])

            # í‰ê·  ê³„ì‚°
            if context_scores:
                stats['avg_context_score'] = round(sum(context_scores) / len(context_scores), 2)

            # Setë¥¼ listë¡œ ë³€í™˜ (JSON ì§ë ¬í™”ë¥¼ ìœ„í•´)
            stats['layers_used'] = list(stats['layers_used'])
            stats['keywords_detected'] = list(stats['keywords_detected'])

            # Trace metadataì— ì¶”ê°€
            if 'metadata' not in trace:
                trace['metadata'] = {}
            trace['metadata'].update(stats)

def main():
    parser = argparse.ArgumentParser(
        description='Aggregate Claude Code and Pipeline logs for LangFuse'
    )
    parser.add_argument(
        '--claude-logs',
        default='.claude/hooks/logs/hook-execution.jsonl',
        help='Path to Claude Code logs'
    )
    parser.add_argument(
        '--pipeline-metrics',
        default='.pipeline-metrics/metrics.jsonl',
        help='Path to Pipeline metrics'
    )
    parser.add_argument(
        '--output',
        default='langfuse/data/langfuse-data.json',
        help='Output file path'
    )
    parser.add_argument(
        '--anonymize',
        action='store_true',
        help='Anonymize sensitive data (usernames, filenames, etc.)'
    )
    parser.add_argument(
        '--telemetry',
        action='store_true',
        help='Enable telemetry mode (auto-read .langfuse.telemetry config)'
    )

    args = parser.parse_args()

    # í…”ë ˆë©”íŠ¸ë¦¬ ëª¨ë“œ: .langfuse.telemetry íŒŒì¼ í™•ì¸
    if args.telemetry:
        telemetry_file = Path('.langfuse.telemetry')
        if not telemetry_file.exists():
            print("âš ï¸  Telemetry mode enabled but .langfuse.telemetry not found")
            print("   Telemetry is disabled. Continuing with normal operation.")
            return

        # í…”ë ˆë©”íŠ¸ë¦¬ ì„¤ì • ì½ê¸°
        telemetry_config = {}
        with open(telemetry_file, 'r') as f:
            for line in f:
                if '=' in line:
                    key, value = line.strip().split('=', 1)
                    telemetry_config[key] = value

        # í…”ë ˆë©”íŠ¸ë¦¬ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ ì¢…ë£Œ
        if telemetry_config.get('enabled', 'false').lower() != 'true':
            print("âš ï¸  Telemetry is disabled in .langfuse.telemetry")
            print("   Skipping telemetry upload.")
            return

        # ìµëª…í™” ê°•ì œ
        args.anonymize = True
        print("ğŸ”’ Telemetry mode: Anonymization enforced")

    print("ğŸš€ LangFuse Log Aggregator")
    print(f"   Claude logs: {args.claude_logs}")
    print(f"   Pipeline metrics: {args.pipeline_metrics}")
    print(f"   Anonymize: {args.anonymize}")

    aggregator = LangFuseAggregator(anonymize=args.anonymize)

    # ë¡œê·¸ ë¡œë“œ
    aggregator.load_claude_logs(args.claude_logs)
    aggregator.load_pipeline_metrics(args.pipeline_metrics)

    # LangFuse í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°
    data = aggregator.export_to_langfuse()

    with open(args.output, 'w') as f:
        json.dump(data, f, indent=2)

    print(f"\nâœ… Export complete!")
    print(f"   Output: {args.output}")
    print(f"   Traces: {len(data['traces'])}")
    print(f"   Observations: {len(data['observations'])}")

if __name__ == '__main__':
    main()
