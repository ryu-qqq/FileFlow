# ===============================================
# Production Profile Configuration - Pipeline Scheduler
# ===============================================
# 프로덕션 환경 전용 설정
# 모든 민감한 정보는 환경변수로 주입
#
# @author FileFlow Team
# @since 1.0.0
# ===============================================

spring:
  # ===============================================
  # DataSource Configuration (Production)
  # ===============================================
  datasource:
    # Terraform에서 주입 (ECS Task Definition)
    # PIPELINE_DB_HOST: ${ssm:/shared/rds/db-instance-address}
    # PIPELINE_DB_PORT: ${ssm:/shared/rds/db-instance-port}
    # PIPELINE_DB_PASSWORD: ${secrets:prod-shared-mysql-master-password}
    url: jdbc:mysql://${PIPELINE_DB_HOST:${DB_HOST}}:${PIPELINE_DB_PORT:${DB_PORT:3306}}/fileflow?useSSL=true&serverTimezone=UTC&characterEncoding=UTF-8
    username: ${PIPELINE_DB_USERNAME:admin}
    password: ${PIPELINE_DB_PASSWORD:${DB_PASSWORD}}
    driver-class-name: com.mysql.cj.jdbc.Driver
    hikari:
      maximum-pool-size: ${PIPELINE_DB_POOL_MAX_SIZE:15}
      minimum-idle: ${PIPELINE_DB_POOL_MIN_IDLE:5}
      connection-timeout: ${PIPELINE_DB_CONNECTION_TIMEOUT:30000}
      idle-timeout: ${PIPELINE_DB_IDLE_TIMEOUT:600000}
      max-lifetime: ${PIPELINE_DB_MAX_LIFETIME:1800000}
      pool-name: PipelineScheduler-Prod-HikariPool
      leak-detection-threshold: 120000
      register-mbeans: true

  # ===============================================
  # Redis Configuration (Production)
  # ===============================================
  data:
    redis:
      # Terraform에서 주입 (ECS Task Definition)
      # REDIS_HOST: ${ssm:/fileflow/prod/redis/endpoint}
      # REDIS_PORT: ${ssm:/fileflow/prod/redis/port}
      host: ${REDIS_HOST}
      port: ${REDIS_PORT:6379}
      password: ${REDIS_PASSWORD:}
      timeout: 3000ms
      lettuce:
        pool:
          max-active: 10
          max-idle: 8
          min-idle: 3

# ===============================================
# AWS S3 Configuration (Production)
# ===============================================
aws:
  s3:
    # Terraform에서 설정: fileflow-prod
    region: ${AWS_REGION:ap-northeast-2}
    bucket-name: ${AWS_S3_BUCKET:fileflow-prod}
    # IAM Role 사용 (ECS Task Role)

  # CloudWatch Logs (Production)
  cloudwatch:
    # Terraform에서 설정: /aws/ecs/fileflow-scheduler-pipeline
    region: ${AWS_REGION:ap-northeast-2}
    log-group: ${AWS_CLOUDWATCH_LOG_GROUP:/aws/ecs/fileflow-scheduler-pipeline}
    log-stream: ${AWS_CLOUDWATCH_LOG_STREAM:pipeline-scheduler-${HOSTNAME}}

# ===============================================
# Pipeline Scheduler Configuration (Production)
# ===============================================
fileflow:
  pipeline:
    outbox:
      fixed-delay: ${PIPELINE_SCHEDULER_FIXED_DELAY:30000}  # 30초
      initial-delay: ${PIPELINE_SCHEDULER_INITIAL_DELAY:10000}
      batch-size: ${PIPELINE_SCHEDULER_BATCH_SIZE:10}
      max-retry-count: ${PIPELINE_SCHEDULER_MAX_RETRY:3}
      retry-base-delay-seconds: 60
      retry-multiplier: 2.0
      retry-max-delay-seconds: 3600
      stale-minutes: 5

# ===============================================
# Actuator Configuration (Production)
# ===============================================
management:
  server:
    port: ${ACTUATOR_PORT:9092}
  endpoints:
    web:
      exposure:
        include: health,metrics,prometheus
  endpoint:
    health:
      show-details: when-authorized
      probes:
        enabled: true
  metrics:
    export:
      prometheus:
        enabled: true
    tags:
      application: ${spring.application.name}
      service: pipeline-scheduler
      environment: production

# ===============================================
# Logging Configuration (Production)
# ===============================================
logging:
  level:
    root: INFO
    com.ryuqq.fileflow: DEBUG  # 스케줄러는 작업 추적을 위해 DEBUG
    org.springframework: INFO
    org.springframework.scheduling: INFO
    org.hibernate.SQL: WARN
    org.hibernate.type.descriptor.sql.BasicBinder: WARN
