# FILEFLOW - SQS Queue

Generated by Infrastructure Wizard on 2025-10-27 17:59:13

## Overview

SQS queue for fileflow with standard delivery and dead letter queue.

## Resources Created

- SQS queue
- Dead letter queue (DLQ)- KMS key for encryption- CloudWatch alarms (depth, age, DLQ)
- SSM parameters (queue URL, ARN, DLQ URL)

## Configuration

- **Queue Type**: Standard- **Message Retention**: 345600 seconds (4 days)
- **Visibility Timeout**:  seconds
- **Long Polling**: 20 seconds
- **Max Message Size**: 256 KB
- **Encryption**: KMS (customer-managed key)- **DLQ**: Enabled (max receive count: 3)
## Security Features

- ðŸ”’ KMS encryption with customer-managed keys
- ðŸ” TLS enforcement (HTTPS only)
- ðŸ›¡ï¸ IAM-based access control
- âš ï¸ Dead letter queue for failed messages
- ðŸ“ CloudWatch alarms for monitoring

## Queue URLs

### Main Queue

```
https://sqs.ap-northeast-2.amazonaws.com/ACCOUNT_ID/fileflow-prod```

### Dead Letter Queue

```
https://sqs.ap-northeast-2.amazonaws.com/ACCOUNT_ID/fileflow-dlq-prod```

### Retrieve from SSM

```bash
QUEUE_URL=$(aws ssm get-parameter --name "/fileflow/prod/sqs/queue-url" --query "Parameter.Value" --output text)
DLQ_URL=$(aws ssm get-parameter --name "/fileflow/prod/sqs/dlq-url" --query "Parameter.Value" --output text)
echo "Queue URL: $QUEUE_URL"
```

## Usage Examples

### AWS CLI

```bash
# Send message
aws sqs send-message \
  --queue-url "$QUEUE_URL" \
  --message-body "Hello World"

# Receive messages
aws sqs receive-message \
  --queue-url "$QUEUE_URL" \
  --max-number-of-messages 10 \
  --wait-time-seconds 20

# Delete message
aws sqs delete-message \
  --queue-url "$QUEUE_URL" \
  --receipt-handle "RECEIPT_HANDLE"

# Purge queue (delete all messages)
aws sqs purge-queue --queue-url "$QUEUE_URL"
```

### Node.js (AWS SDK v3)

```javascript
const { SQSClient, SendMessageCommand, ReceiveMessageCommand, DeleteMessageCommand } = require('@aws-sdk/client-sqs');

const client = new SQSClient({ region: 'ap-northeast-2' });
const queueUrl = process.env.QUEUE_URL;

// Send message
async function sendMessage(body) {
  const command = new SendMessageCommand({
    QueueUrl: queueUrl,
    MessageBody: JSON.stringify(body),
  });

  const response = await client.send(command);
  console.log('Message sent:', response.MessageId);
}

// Receive messages
async function receiveMessages() {
  const command = new ReceiveMessageCommand({
    QueueUrl: queueUrl,
    MaxNumberOfMessages: 10,
    WaitTimeSeconds: 20,
  });

  const response = await client.send(command);
  return response.Messages || [];
}

// Delete message
async function deleteMessage(receiptHandle) {
  const command = new DeleteMessageCommand({
    QueueUrl: queueUrl,
    ReceiptHandle: receiptHandle,
  });

  await client.send(command);
}

// Process messages
async function processMessages() {
  const messages = await receiveMessages();

  for (const message of messages) {
    try {
      const body = JSON.parse(message.Body);
      console.log('Processing:', body);

      // Your processing logic here

      await deleteMessage(message.ReceiptHandle);
    } catch (error) {
      console.error('Error processing message:', error);
    }
  }
}
```

### Python (boto3)

```python
import boto3
import json
import os

sqs = boto3.client('sqs', region_name='ap-northeast-2')
queue_url = os.getenv('QUEUE_URL')

# Send message
def send_message(body):
    response = sqs.send_message(
        QueueUrl=queue_url,
        MessageBody=json.dumps(body),
    )
    print(f"Message sent: {response['MessageId']}")

# Receive messages
def receive_messages():
    response = sqs.receive_message(
        QueueUrl=queue_url,
        MaxNumberOfMessages=10,
        WaitTimeSeconds=20,
    )
    return response.get('Messages', [])

# Delete message
def delete_message(receipt_handle):
    sqs.delete_message(
        QueueUrl=queue_url,
        ReceiptHandle=receipt_handle,
    )

# Process messages
def process_messages():
    messages = receive_messages()

    for message in messages:
        try:
            body = json.loads(message['Body'])
            print(f"Processing: {body}")

            # Your processing logic here

            delete_message(message['ReceiptHandle'])
        except Exception as e:
            print(f"Error processing message: {e}")
```

### Go (AWS SDK v2)

```go
import (
    "context"
    "encoding/json"
    "github.com/aws/aws-sdk-go-v2/aws"
    "github.com/aws/aws-sdk-go-v2/service/sqs"
)

client := sqs.NewFromConfig(cfg)
queueURL := os.Getenv("QUEUE_URL")

// Send message
func sendMessage(ctx context.Context, body interface{}) error {
    data, _ := json.Marshal(body)

    input := &sqs.SendMessageInput{
        QueueUrl:    aws.String(queueURL),
        MessageBody: aws.String(string(data)),
    }

    result, err := client.SendMessage(ctx, input)
    if err != nil {
        return err
    }

    fmt.Printf("Message sent: %s\n", *result.MessageId)
    return nil
}

// Receive messages
func receiveMessages(ctx context.Context) ([]types.Message, error) {
    input := &sqs.ReceiveMessageInput{
        QueueUrl:            aws.String(queueURL),
        MaxNumberOfMessages: 10,
        WaitTimeSeconds:     20,
    }

    result, err := client.ReceiveMessage(ctx, input)
    if err != nil {
        return nil, err
    }

    return result.Messages, nil
}

// Delete message
func deleteMessage(ctx context.Context, receiptHandle string) error {
    input := &sqs.DeleteMessageInput{
        QueueUrl:      aws.String(queueURL),
        ReceiptHandle: aws.String(receiptHandle),
    }

    _, err := client.DeleteMessage(ctx, input)
    return err
}
```

## Monitoring

### CloudWatch Metrics

```bash
# Queue depth
aws cloudwatch get-metric-statistics \
  --namespace AWS/SQS \
  --dimensions Name=QueueName,Value=fileflow-prod \
  --metric-name ApproximateNumberOfMessagesVisible \
  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \
  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
  --period 300 \
  --statistics Average

# Message age
aws cloudwatch get-metric-statistics \
  --namespace AWS/SQS \
  --dimensions Name=QueueName,Value=fileflow-prod \
  --metric-name ApproximateAgeOfOldestMessage \
  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \
  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
  --period 300 \
  --statistics Maximum
```

### CloudWatch Alarms

- **Queue Depth**: Alarm when > 1000 messages for 10 minutes
- **Message Age**: Alarm when oldest message > 1 hour
- **DLQ Depth**: Alarm when any message appears in DLQ
## Best Practices

### Message Processing

1. **Use Long Polling**: Already configured (20 seconds)
2. **Handle Idempotency**: Process messages idempotently
3. **Visibility Timeout**: Set based on processing time (currently s)
4. **Delete After Processing**: Always delete successfully processed messages


### Error Handling

1. **Retry Logic**: Implement exponential backoff
2. **DLQ Monitoring**: Monitor DLQ for failed messages3. **Logging**: Log all processing errors
4. **Dead Letter Analysis**: Regularly review and reprocess DLQ messages

## Troubleshooting

### High Queue Depth

1. Scale up consumers
2. Optimize message processing
3. Check for processing errors

### Messages in DLQ

```bash
# Check DLQ depth
aws sqs get-queue-attributes \
  --queue-url "$DLQ_URL" \
  --attribute-names ApproximateNumberOfMessages

# Receive DLQ messages
aws sqs receive-message --queue-url "$DLQ_URL"

# Reprocess DLQ messages
aws sqs send-message-batch \
  --queue-url "$QUEUE_URL" \
  --entries file://messages.json
```

## Deployment

1. Initialize Terraform: `terraform init`
2. Plan changes: `terraform plan`
3. Apply: `terraform apply`

---

ðŸ¤– Generated with Infrastructure Wizard
