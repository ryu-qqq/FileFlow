#!/usr/bin/env python3
"""
AI Review Deduplicator

ì—¬ëŸ¬ AI ë´‡ì˜ ëŒ“ê¸€ ì¤‘ë³µ ì œê±°
- íŒŒì¼:ë¼ì¸ ìœ„ì¹˜ ë§¤ì¹­
- í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° (TF-IDF ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
- ì¹´í…Œê³ ë¦¬ ë§¤ì¹­
- Similarity > 0.8 ì‹œ ë³‘í•©
"""

import re
from typing import List, Dict, Set, Tuple
from dataclasses import dataclass, field
from collections import Counter
import math


@dataclass
class MergedIssue:
    """ë³‘í•©ëœ ì´ìŠˆ"""
    id: str  # ëŒ€í‘œ ID
    file: str
    line: int
    category: str
    description: str
    bots: List[str] = field(default_factory=list)  # ì´ ì´ìŠˆë¥¼ ì œê¸°í•œ ë´‡ë“¤
    vote_count: int = 0  # íˆ¬í‘œ ìˆ˜ (1-3)
    priority: str = ""  # Critical, Important, Suggestion
    similarity_score: float = 1.0  # ë³‘í•©ëœ ëŒ“ê¸€ë“¤ì˜ í‰ê·  ìœ ì‚¬ë„


class Deduplicator:
    """ì¤‘ë³µ ì œê±° ë° ë³‘í•© í´ë˜ìŠ¤"""

    SIMILARITY_THRESHOLD = 0.8  # ìœ ì‚¬ë„ ì„ê³„ê°’

    def __init__(self, comments: List[Dict]):
        """
        ì´ˆê¸°í™”

        Args:
            comments: fetch_reviews.pyì—ì„œ ê°€ì ¸ì˜¨ ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸
        """
        self.comments = comments
        self.merged_issues: List[MergedIssue] = []

    def deduplicate(self) -> List[MergedIssue]:
        """
        ì¤‘ë³µ ì œê±° ë° ë³‘í•© ì‹¤í–‰

        Returns:
            ë³‘í•©ëœ ì´ìŠˆ ë¦¬ìŠ¤íŠ¸
        """
        print(f"\nğŸ”„ ì¤‘ë³µ ì œê±° ì‹œì‘: {len(self.comments)}ê°œ ëŒ“ê¸€")

        # 1. ìœ„ì¹˜ë³„ë¡œ ê·¸ë£¹í™” (file:line)
        location_groups = self._group_by_location()

        # 2. ê° ê·¸ë£¹ ë‚´ì—ì„œ ìœ ì‚¬ë„ ê³„ì‚° ë° ë³‘í•©
        for location, group_comments in location_groups.items():
            if len(group_comments) == 1:
                # ë‹¨ì¼ ëŒ“ê¸€ â†’ ë°”ë¡œ ì¶”ê°€
                comment = group_comments[0]
                self.merged_issues.append(self._create_merged_issue([comment]))
            else:
                # ë‹¤ì¤‘ ëŒ“ê¸€ â†’ ìœ ì‚¬ë„ ê³„ì‚° í›„ ë³‘í•©
                merged = self._merge_similar_comments(group_comments)
                self.merged_issues.extend(merged)

        # 3. íˆ¬í‘œ ìˆ˜ ê³„ì‚°
        for issue in self.merged_issues:
            issue.vote_count = len(set(issue.bots))

        print(f"âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(self.merged_issues)}ê°œ ì´ìŠˆ")
        print(f"  ë³‘í•©ë¥ : {((len(self.comments) - len(self.merged_issues)) / len(self.comments) * 100):.1f}%")

        return self.merged_issues

    def _group_by_location(self) -> Dict[str, List[Dict]]:
        """íŒŒì¼:ë¼ì¸ ìœ„ì¹˜ë³„ë¡œ ëŒ“ê¸€ ê·¸ë£¹í™”"""
        groups = {}

        for comment in self.comments:
            file = comment.get("file") or "general"
            line = comment.get("line") or 0
            location_key = f"{file}:{line}"

            if location_key not in groups:
                groups[location_key] = []
            groups[location_key].append(comment)

        return groups

    def _merge_similar_comments(self, comments: List[Dict]) -> List[MergedIssue]:
        """
        ìœ ì‚¬í•œ ëŒ“ê¸€ ë³‘í•©

        Args:
            comments: ê°™ì€ ìœ„ì¹˜ì˜ ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸

        Returns:
            ë³‘í•©ëœ ì´ìŠˆ ë¦¬ìŠ¤íŠ¸
        """
        merged_groups: List[List[Dict]] = []
        used_indices: Set[int] = set()

        for i, comment1 in enumerate(comments):
            if i in used_indices:
                continue

            current_group = [comment1]
            used_indices.add(i)

            for j, comment2 in enumerate(comments[i+1:], start=i+1):
                if j in used_indices:
                    continue

                similarity = self._calculate_similarity(comment1, comment2)
                if similarity > self.SIMILARITY_THRESHOLD:
                    current_group.append(comment2)
                    used_indices.add(j)

            merged_groups.append(current_group)

        return [self._create_merged_issue(group) for group in merged_groups]

    def _calculate_similarity(self, comment1: Dict, comment2: Dict) -> float:
        """
        ë‘ ëŒ“ê¸€ ê°„ ìœ ì‚¬ë„ ê³„ì‚°

        Args:
            comment1: ì²« ë²ˆì§¸ ëŒ“ê¸€
            comment2: ë‘ ë²ˆì§¸ ëŒ“ê¸€

        Returns:
            ìœ ì‚¬ë„ (0.0-1.0)
        """
        # 1. ìœ„ì¹˜ ìœ ì‚¬ë„ (ê°™ì€ ìœ„ì¹˜ë©´ 1.0)
        location_sim = 1.0 if (
            comment1.get("file") == comment2.get("file") and
            comment1.get("line") == comment2.get("line")
        ) else 0.0

        # 2. í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ (TF-IDF ì½”ì‚¬ì¸)
        text1 = comment1.get("body", "")
        text2 = comment2.get("body", "")
        text_sim = self._cosine_similarity(text1, text2)

        # 3. ì¹´í…Œê³ ë¦¬ ìœ ì‚¬ë„
        category_sim = 1.0 if comment1.get("category") == comment2.get("category") else 0.0

        # ê°€ì¤‘ í‰ê·  (ìœ„ì¹˜ 50%, í…ìŠ¤íŠ¸ 30%, ì¹´í…Œê³ ë¦¬ 20%)
        similarity = (
            location_sim * 0.5 +
            text_sim * 0.3 +
            category_sim * 0.2
        )

        return similarity

    def _cosine_similarity(self, text1: str, text2: str) -> float:
        """
        TF-IDF ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨ ë²„ì „)

        Args:
            text1: ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸
            text2: ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸

        Returns:
            ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (0.0-1.0)
        """
        # í† í°í™” (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ê¸°ì¤€)
        tokens1 = self._tokenize(text1.lower())
        tokens2 = self._tokenize(text2.lower())

        if not tokens1 or not tokens2:
            return 0.0

        # TF (Term Frequency)
        tf1 = Counter(tokens1)
        tf2 = Counter(tokens2)

        # ê³µí†µ í† í°
        common_tokens = set(tf1.keys()) & set(tf2.keys())

        if not common_tokens:
            return 0.0

        # ë‚´ì  ê³„ì‚°
        dot_product = sum(tf1[token] * tf2[token] for token in common_tokens)

        # í¬ê¸° ê³„ì‚°
        magnitude1 = math.sqrt(sum(count ** 2 for count in tf1.values()))
        magnitude2 = math.sqrt(sum(count ** 2 for count in tf2.values()))

        if magnitude1 == 0 or magnitude2 == 0:
            return 0.0

        return dot_product / (magnitude1 * magnitude2)

    def _tokenize(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ í† í°í™”

        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸

        Returns:
            í† í° ë¦¬ìŠ¤íŠ¸
        """
        # ì•ŒíŒŒë²³, ìˆ«ìë§Œ ì¶”ì¶œ
        tokens = re.findall(r'\b\w+\b', text)
        # ë¶ˆìš©ì–´ ì œê±° (ê°„ë‹¨ ë²„ì „)
        stopwords = {"the", "a", "an", "is", "it", "to", "of", "and", "in", "for"}
        return [token for token in tokens if token not in stopwords and len(token) > 2]

    def _create_merged_issue(self, comments: List[Dict]) -> MergedIssue:
        """
        ëŒ“ê¸€ ê·¸ë£¹ì„ ë³‘í•©ëœ ì´ìŠˆë¡œ ë³€í™˜

        Args:
            comments: ë³‘í•©í•  ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸

        Returns:
            ë³‘í•©ëœ ì´ìŠˆ
        """
        # ëŒ€í‘œ ëŒ“ê¸€ (ì²« ë²ˆì§¸)
        representative = comments[0]

        # ëª¨ë“  ë´‡ ìˆ˜ì§‘
        bots = list(set(comment.get("bot_name", "unknown") for comment in comments))

        # ì„¤ëª… ë³‘í•© (ê°€ì¥ ê¸´ ê²ƒ ì„ íƒ)
        descriptions = [comment.get("body", "") for comment in comments]
        merged_description = max(descriptions, key=len)

        return MergedIssue(
            id=representative.get("id", "unknown"),
            file=representative.get("file", "general"),
            line=representative.get("line", 0),
            category=representative.get("category", "general"),
            description=merged_description,
            bots=bots,
            vote_count=len(set(bots)),
            similarity_score=1.0 if len(comments) == 1 else 0.9  # ì„ì‹œ
        )

    def show_deduplication_report(self) -> None:
        """ì¤‘ë³µ ì œê±° ë¦¬í¬íŠ¸ ì¶œë ¥"""
        print("\nğŸ“Š ì¤‘ë³µ ì œê±° ë¦¬í¬íŠ¸")
        print("=" * 60)
        print(f"ì›ë³¸ ëŒ“ê¸€ ìˆ˜: {len(self.comments)}")
        print(f"ë³‘í•© í›„ ì´ìŠˆ ìˆ˜: {len(self.merged_issues)}")
        print(f"ë³‘í•©ë¥ : {((len(self.comments) - len(self.merged_issues)) / len(self.comments) * 100):.1f}%")

        # íˆ¬í‘œ ë¶„í¬
        vote_distribution = Counter(issue.vote_count for issue in self.merged_issues)
        print("\níˆ¬í‘œ ë¶„í¬:")
        for vote_count in sorted(vote_distribution.keys(), reverse=True):
            count = vote_distribution[vote_count]
            print(f"  {vote_count}ë´‡ í•©ì˜: {count}ê°œ")


if __name__ == "__main__":
    """í…ŒìŠ¤íŠ¸ìš©"""
    import json
    import sys

    if len(sys.argv) < 2:
        print("Usage: python deduplicator.py <comments.json>")
        sys.exit(1)

    with open(sys.argv[1], 'r', encoding='utf-8') as f:
        data = json.load(f)
        comments = data.get("comments", [])

    dedup = Deduplicator(comments)
    merged_issues = dedup.deduplicate()
    dedup.show_deduplication_report()

    # ê²°ê³¼ ì¶œë ¥
    print("\në³‘í•©ëœ ì´ìŠˆ:")
    for issue in merged_issues[:5]:  # ì²˜ìŒ 5ê°œë§Œ
        print(f"\n{issue.file}:{issue.line} ({issue.category})")
        print(f"  ë´‡: {', '.join(issue.bots)} (íˆ¬í‘œ: {issue.vote_count})")
        print(f"  ì„¤ëª…: {issue.description[:100]}...")
